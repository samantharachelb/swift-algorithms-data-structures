# Big-O Notation

Big O notation is a mathematical model used to describe the complexity of
algorithms as a function of their input size.

##### Constant Time - O(1)

Execution time remains the same regardless of the size of the input data

##### Linear Time - O(n)

Execution time grows linearly in direct proportion to the size of the input data.

##### Quadratic Time - O(n^2)

Execution time is proportionate to the square of the size of the input data

###### Cubic Time - O(n^3)

Execution time is proportionate to the cube of the size of the input data

###### Quartic Time - O(n^4)

Execution time is proportionate to the 4th power of the size of the input data

##### Logarithmic Time - O(log n)
